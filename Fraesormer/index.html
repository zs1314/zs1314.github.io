<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">
  <style>
    .grid-container {
      display: grid;
      grid-template-columns: repeat(5, 1fr); /* 5列 */
      grid-template-rows: repeat(4, 1fr); /* 4行 */
      gap: 10px; /* 网格项之间的间隙 */
    }

    .grid-item {
      border: 1px solid #ccc; /* 边框，可以根据需要调整 */
      padding: 10px; /* 内边距，可以根据需要调整 */
      display: flex;
      justify-content: center;
      align-items: center;
    }

    /* 可以根据视频类型添加不同的样式 */
    .type1 { background-color: lightblue; }
    .type2 { background-color: lightcoral; }
    .type3 { background-color: lightgreen; }
    .type4 { background-color: lightyellow; }
    .type5 { background-color: lightpink; }
    .video-container {
        display: flex; /* 使用Flexbox布局 */
        justify-content: center; /* 水平居中 */
        margin-bottom: 20px;
    }
      .task-title {
          text-align: center;
          margin: 20px 0;
      }
    .container {
      width: 90%;
      padding: 1; /* 去除左右内边距 */
      margin: 20px 0; /* 水平居中 */
      flex: 1;
      display: flex;
      flex-direction: column;
      flex-wrap: wrap;
    }

    .thumbnail-container {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      gap: 15px;
    }
  
    .thumbnail-container .viewer {
      /* width: 100%; */
      aspect-ratio: 1; /* 强制保持正方形比例 */
      background: #ccc; /* 灰色背景 */
      position: relative;
      border: 2px solid transparent;
      transition: border-color 0.3s;
    }
  
    .thumbnail-container .viewer.selected {
      border-color: black;
    }
  
    #interactive-container {
      display: none; /* 默认隐藏 */
      justify-content: space-between;
    }

    #interactive-viewer, #info-box {
      width: 50%;
      aspect-ratio: 1; /* 强制保持正方形比例 */
      background: #ccc;
      margin: 20px 0; /* 顶部和底部的间距 */
      float: left; /* 左对齐 */
      position: relative;
    }

    #info-box {
      background: #f0f0f0; /* 更浅的背景颜色区分内容 */
      margin: 20px 0; /* 顶部和底部的间距 */
      display: flex;
      justify-content: center;
      align-items: center;
      padding: 10px;
      box-sizing: border-box;
    }
  
    </style>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition"
    />
    <meta name="keywords" content="Diffusion Model, Skill Discovery, Trajectory Generation, Multitask Learning" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition
    </title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap"
      rel="stylesheet"
    />
    <link href="./public/index.css" rel="stylesheet" />
    <link href="./public/media.css" rel="stylesheet" />
    <link href="./public/sidebars.css" rel="stylesheet" />
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    <script src="./public/js/base.js"></script>
  </head>

  <body>
    <div class="sidebarsWrapper">
      <div class="sidebars">
        <a class="barWrapper" clear href="#abstract-a" id="bar2"
          ><span>Abstract</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#pipeline" id="bar3"
          ><span>Pipeline of Fraesormer</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#visualization" id="bar4"
        ><span>Visualization</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#Quantitative results" id="bar8"
          ><span>Quantitative results</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#Seven-dimensional radar map" id="bar8"
          ><span>Seven-dimensional radar map</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#Ablation Study" id="bar8"
          ><span>Ablation Study</span>
          <div class="bar"></div
        ></a>
        <a class="barWrapper" clear href="#Bibtex" id="bar9"
          ><span>Bibtex</span>
          <div class="bar"></div
        ></a>
      </div>
    </div>

    <!-- \definecolor{hku_color}{HTML}{13A983} % 注意HTML颜色代码是大写的
\definecolor{szu_color}{HTML}{84193E} % 同上 -->
    <main class="content">
      <div style="display: flex; margin: auto; width: 100%; height: auto; align-content: center; align-items: center;  justify-content: center;">
          <img
          style="width: 30%; height: auto"
          src="files/logo.png"
          />
        </div>
      <section class="heading" style="text-align: center!important;">
        <h1 class="title">
          Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition<br>
          <!-- <img src="files/institute.png" style="width: 100%; height: auto;">  -->
        </h1>
        
        <section class="authors">
          <ul>
            <li>
              <span
                ><a
                  href="https://zs1314.github.io/"
                  rel="noreferrer"
                  target="_blank"
              >Shun Zou</a
                > <sup></sup></span
              >,
              <span
                ><a
                  href="https://zs1314.github.io/Fraesormer/"
                  rel="noreferrer"
                  target="_blank"
              >Yi Zou</a
                > <sup></sup></span
              >,
              <span
                ><a
                  href="https://scholar.google.com/citations?hl=en&user=vZy9f-AAAAAJ&view_op=list_works&sortby=pubdate"
                  rel="noreferrer"
                  target="_blank"
              >Mingya Zhang</a
                > <sup></span
              >,
              <span
                ><a
                  href="https://zs1314.github.io/Fraesormer/"
                  rel="noreferrer"
                  target="_blank"
              >Shipeng Luo</a
                > <sup></span
              >,
              <span
                ><a
                  href="https://scholar.google.com/citations?user=SBoHvVQAAAAJ&hl=en"
                  rel="noreferrer"
                  target="_blank"
              >Zhihao Chen</a
                > <sup></span
              >,
              <span
                ><a
                  href="https://guangweigao.github.io/"
                  rel="noreferrer"
                  target="_blank"
              >Guangwei Gao</a
                > <sup>&#8224;</span
              >
            </li>
        </section>
        <!-- <section class="affiliations">
          <ul>
            <li><sup>1</sup> The University of Hong Kong,</li>
            <li><sup>2</sup> Institute of Artificial Intelligence (TeleAI), China Telecom,</li><br>
            <li><sup>3</sup> Shenzhen University,</li>
            <li><sup>4</sup> AgileX Robotics,</li><br>
            <li><sup>5</sup> Guangdong Institute of Intelligence Science and Technology</li>
          </ul>
        </section> -->
        <section class="corresponding">
          <p>
            <sup>*</sup>Equal Contribution, <sup>&#8224;</sup>Corresponding authors
          </p>
          <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Ftianxingchen.github.io%2FG3Flow&count_bg=%23388FD7&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=Viewers&edge_flat=false"/></a>

        </section>
        <!-- <section class="conference">
          <h3>
          Under Review
          </h3>
        </section> -->
        <section class="links">
          <ul>
            <a href="http://arxiv.org/abs/2503.11995" rel="noreferrer" target="_blank">
              <li>
                <span class="icon"> <img src="./public/paper.svg" /> </span
                ><span>arXiv</span>
              </li>
            </a>
            <a
              href="https://arxiv.org/pdf/2503.11995"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon"> <img src="./public/poster.svg" style="width: 120%;filter: invert(100%); stroke-width: 200%"/> </span
                ><span>PDF</span>
              </li>
            </a>
            <a
              href="https://github.com/zs1314/Fraesormer"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon">
                  <img src="./public/github.svg" />
                </span>
                <span>Code</span>
              </li>
            </a>
            <!-- <a
              href="https://www.youtube.com/watch?v=veQI1iuvYZA"
              rel="noreferrer"
              target="_blank"
             >
               <li><span class="icon"> <img src="./public/video.svg"/> </span
                ><span>Video</span>
               </li>
             </a> -->
            <!-- <a
              href="https://huggingface.co/datasets/YaoMarkMu/robotwin_dataset/tree/main"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon"> <img src="./public/database2.svg" style="width: 100%;filter: invert(100%); stroke-width: 200%; padding-bottom:2.5px"/> </span
                ><span>Dataset</span>
              </li>
            </a> -->
            
          </ul>
        </section>
        <a class="anchor" id="abstract-a"></a>
        <!-- <img src="./files/d-robotics.jpg" style="width: 10%"> -->
        <h2>Abstract</h2>
        <p class="abstract" style="font-family: 'Times New Roman', Arial; text-align: justify">
        In recent years, Transformer has witnessed significant progress in food recognition. However, most existing
        approaches still face two critical challenges in lightweight food
        recognition: (1) the quadratic complexity and redundant feature
        representation from interactions with irrelevant tokens; (2)
        static feature recognition and single-scale representation, which
        overlook the unstructured, non-fixed nature of food images
        and the need for multi-scale features. To address these, we
        propose an adaptive and efficient sparse Transformer architecture
        (Fraesormer) with two core designs: Adaptive Top-k Sparse Partial
        Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature
        Gating Network (HSSFGN). ATK-SPA uses a learnable Gated
        Dynamic Top-K Operator (GDTKO) to retain critical attention
        scores, filtering low query-key matches that hinder feature
        aggregation. It also introduces a partial channel mechanism
        to reduce redundancy and promote expert information flow,
        enabling local-global collaborative modeling. HSSFGN employs
        gating mechanism to achieve multi-scale feature representation,
        enhancing contextual semantic information. Extensive experiments
        show that Fraesormer outperforms state-of-the-art methods.
        </p>
      </section>

      <a class="anchor" id="pipeline"></a>
      <section class="details" style="text-align: justify;">
        <div style="display: flex; flex-direction: column; align-items: center; width: 100%; height: 100%;">
          <h2>Overall Pipeline</h2>
        </div>
        <div style="display: flex; margin: auto; width: 100%; height: auto; align-content: center; align-items: center;  justify-content: center;">
          <img
          style="width: 90%; height: auto"
          src="files/main.png"
          />
        </div>
        <br />
<!--        <p style="font-family: 'Times New Roman',serif"> <strong>Pipeline of OCTAMamba.</strong> Our framework consists of (top) an initialization phase that generates comprehensive 3D representation (surface normals, wireframe, and geometry) through object-centric exploration and digital twin generation, which enables rich semantic field extraction, and (bottom) a control execution phase where real-time pose tracking maintains dynamic semantic fields to guide diffusion-based manipulation actions for pose-aware and generalizable manipulation.-->
<!--        </p>-->
        <!-- ----------------------------- -->
        <a class="anchor" id="tasks-a"></a>

        
        <a class="anchor" id="visualization"></a>
        <div style="display: flex; flex-direction: column; align-items: center; width: 100%; height: 100%;">
          <h2>Visualization</h2>
        </div>
        <div style="display: flex; margin: auto; width: 100%; height: auto; align-content: center; align-items: center;  justify-content: center;">
          <img
          style="width: 100%; height: auto"
          src="files/visualization.png"
          />
        </div>
        <p style="font-family: 'Times New Roman',serif">Visualization of the impact of each spatial location on the final prediction of the DeiT-S model. The results show that the final prediction of the vision transformer is primarily based on the most influential tokens, indicating that a large portion of tokens can be removed without affecting performance.</p>

        <br><br>
      <a class="anchor" id="Quantitative results" style="margin-top: -2em"></a>
      <section class="details" style="text-align: justify;">
        <div style="display: flex; flex-direction: column; align-items: center; width: 100%; height: 100%;">
          <h2>Quantitative results</h2>
        </div>
<!--        <p style="font-family: 'Times New Roman'"> Qualitative visualization of different methods. Best viewed by zooming in the figures on high-resolution displays.</p>-->
        
        <div style="display: flex; margin: auto; width: 100%; height: auto; align-content: center; align-items: center;  justify-content: center;">
          <img
          style="width: 70%; height: auto"
          src="files/compare1.png"
          />
        </div>
        <br />
        <br><br>
      <a class="anchor" id="Seven-dimensional radar map" style="margin-top: -2em"></a>
      <section class="details" style="text-align: justify;">
        <div style="display: flex; flex-direction: column; align-items: center; width: 100%; height: 100%;">
          <h2>Seven-dimensional radar map</h2>
        </div>
<!--        <p style="font-family: 'Times New Roman'"> Qualitative visualization of different methods. Best viewed by zooming in the figures on high-resolution displays.</p>-->

        <div style="display: flex; margin: auto; width: 100%; height: auto; align-content: center; align-items: center;  justify-content: center;">
          <img
          style="width: 70%; height: auto"
          src="files/compare2.png"
          />
        </div>
        <br />
        <br><br>
        <a class="anchor" id="Ablation Study" style="margin-top: -2em"></a>
        <section class="details" style="text-align: justify;">
          <div style="display: flex; flex-direction: column; align-items: center; width: 100%; height: 100%;">
            <h2>Ablation Study</h2>
          </div>

<!--        <p>To explore the impact of each component on model performance, we conducted ablation experiments on ROSSA 3M. From Table II, it is evident that QSEME, MSDAM, and FFRM all enhanced the model’s segmentation of the target area to varying degrees. The performance of OCATMamba was optimal when all three modules were used simultaneously.</p>-->
        <div style="display: flex; margin: auto; width: 100%; height: auto; align-content: center; align-items: center;  justify-content: center;">
          <img
          style="width: 100%; height: auto"
          src="files/ablation.png"
          />
        </div>

        <a class="anchor" id="Bibtex"></a>

      <section class="citation" style="text-align: justify;">
        <div style="display: flex; flex-direction: column; align-items: center; width: 100%; height: 100%;">
          <h2>Bibtex</h2>
        </div>
        <pre>
      <code>
@article{zou2025fraesormer,
  title={Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition},
  author={Zou, Shun and Zou, Yi and Zhang, Mingya and Luo, Shipeng and Chen, Zhihao and Gao, Guangwei},
  journal={},
  year={2025}
}
      </code></pre>
      </section>
      <br/>



      <script type="importmap">
        {
          "imports": {
            "three": "https://cdn.jsdelivr.net/npm/three@0.150.1/build/three.module.js",
            "OrbitControls": "https://cdn.jsdelivr.net/npm/three@0.150.1/examples/jsm/controls/OrbitControls.js",
            "OBJLoader": "https://cdn.jsdelivr.net/npm/three@0.150.1/examples/jsm/loaders/OBJLoader.js",
            "GLTFLoader": "https://cdn.jsdelivr.net/npm/three@0.150.1/examples/jsm/loaders/GLTFLoader.js",
            "MTLLoader": "https://cdn.jsdelivr.net/npm/three@0.150.1/examples/jsm/loaders/MTLLoader.js"
          }
        }
      </script>
    
      <script type="module">
        import * as THREE from 'three';
        import { OrbitControls } from 'OrbitControls';
        import { OBJLoader } from 'OBJLoader';
        import { GLTFLoader } from 'GLTFLoader';
        import { MTLLoader } from 'MTLLoader';
    
        document.addEventListener("DOMContentLoaded", function () {
          const models = ['024_brush', '028_dustpan', '019_coaste', '022_cup'];
          const scalesize = ["0.7", "1.2", "1.2", "1", "1", "0.5", "0.5", "1"];
          const container = document.getElementById("3d-models");
          const interactiveViewer = document.getElementById("interactive-viewer");
          const infoBox = document.getElementById("info-box");
    
          let currentModel = models[0];
          let interactiveRenderer = null;
    
          models.forEach((model, index) => {
            const viewerDiv = document.createElement("div");
            viewerDiv.className = "viewer";
            viewerDiv.dataset.model = model;
            // viewerDiv.addEventListener("click", () => updateInteractiveViewer(model, index, viewerDiv));
            container.appendChild(viewerDiv);
            if (index < 5)
              loadModel_mtl(model, index, viewerDiv);
            else loadModel_glb(model, index, viewerDiv);
            // loadModel_glb(model, index, viewerDiv);
            // if (index === 0) {
            //   viewerDiv.click();
            //   updateInfoBox(model, index);
            // }
          });
    
          function updateInteractiveViewer(model, index, selectedViewer) {
            document.getElementById('interactive-container').style.display = 'block';
            currentModel = model;
            const previousSelected = container.querySelector(".viewer.selected");
            if (previousSelected) {
              previousSelected.classList.remove("selected");
            }
            selectedViewer.classList.add("selected");
            if (index < 5)
            loadInteractiveModel_mtl(model, index, interactiveViewer);
            else loadInteractiveModel_glb(model, index, interactiveViewer);
            // loadInteractiveModel_glb(model, index, interactiveViewer);
            updateInfoBox(model, index);
          }
    
          function updateInfoBox(model) {
            infoBox.innerHTML = `<p>Model: ${model} <br> Size: (H, W, L) <br> Real Object Purchase Link: <a href="https://tianxingchen.github.io">link</a> <br> Files Download link: <a href="https://tianxingchen.github.io">link</a> </p>`;
          }
    
          function loadModel_mtl(model, index, viewer) {
            const scene = new THREE.Scene();
            const camera = new THREE.PerspectiveCamera(50, viewer.offsetWidth / viewer.offsetHeight, 0.1, 1000);
            const renderer = new THREE.WebGLRenderer({ antialias: true });
            renderer.setSize(viewer.offsetWidth, viewer.offsetHeight);
            viewer.appendChild(renderer.domElement);
            renderer.setClearColor(0xCCCCCC);
    
            const ambientLight = new THREE.AmbientLight(0x404040, 8);
            scene.add(ambientLight);
    
            const directionalLight = new THREE.DirectionalLight(0xffffff, 8);
            directionalLight.position.set(0, 1, 1).normalize();
            scene.add(directionalLight);
    
            const mtlLoader = new MTLLoader();
            mtlLoader.setPath(`files/obj/${model}/`);
            mtlLoader.load('base.mtl', (materials) => {
              materials.preload();
              const objLoader = new OBJLoader();
              objLoader.setMaterials(materials);
              objLoader.setPath(`files/obj/${model}/`);
              objLoader.load('textured.obj', function (object) {
                scene.add(object);
                object.position.set(0, 0, 0);
    
                const box = new THREE.Box3().setFromObject(object);
                const center = box.getCenter(new THREE.Vector3());
                const size = box.getSize(new THREE.Vector3());
    
                const maxDim = Math.max(size.x, size.y, size.z);
                const scale = 1 / maxDim;
                object.scale.set(scalesize[index], scalesize[index], scalesize[index]);
    
                camera.position.set(center.x, center.y + size.z * 2, size.z * 2);
                camera.lookAt(center);
    
                function animate() {
                  requestAnimationFrame(animate);
                  object.rotation.y += 0.01;
                  renderer.render(scene, camera);
                }
                animate();
              }, undefined, function (error) {
                console.error(`Error loading OBJ file: ${error}`);
              });
            }, undefined, function (error) {
              console.error(`Error loading MTL file: ${error}`);
            });
    
            camera.position.z = 2;
          }
    
          function loadModel_glb(model, index, viewer) {
            const scene = new THREE.Scene();
            const camera = new THREE.PerspectiveCamera(50, viewer.offsetWidth / viewer.offsetHeight, 0.1, 1000);
            const renderer = new THREE.WebGLRenderer({ antialias: true });
            renderer.setSize(viewer.offsetWidth, viewer.offsetHeight);
            renderer.outputEncoding = THREE.sRGBEncoding;
            viewer.appendChild(renderer.domElement);
            renderer.setClearColor(0xCCCCCC);
    
            const ambientLight = new THREE.AmbientLight(0x404040, 5);
            scene.add(ambientLight);
    
            const directionalLight = new THREE.DirectionalLight(0xffffff, 2);
            directionalLight.position.set(0, 1, 1).normalize();
            scene.add(directionalLight);
    
            const gltfLoader = new GLTFLoader();
            gltfLoader.load(`files/glb/${model}/base.glb`, function (gltf) {
              const object = gltf.scene;
              scene.add(object);
              object.position.set(0, 0, 0);
              object.traverse((node) => {
                if (node.isMesh) {
                  node.material = new THREE.MeshStandardMaterial({
                    map: node.material.map,
                    roughness: node.material.roughness,
                    metalness: node.material.metalness,
                  });
                }
              });
              const box = new THREE.Box3().setFromObject(object);
              const center = box.getCenter(new THREE.Vector3());
              const size = box.getSize(new THREE.Vector3());
    
              const maxDim = Math.max(size.x, size.y, size.z);
              const scale = 1 / maxDim;
              object.scale.set(scalesize[index], scalesize[index], scalesize[index]);
    
              camera.position.set(center.x, center.y + size.z * 2, size.z * 2);
              camera.lookAt(center);
    
              function animate() {
                requestAnimationFrame(animate);
                object.rotation.y += 0.01;
                renderer.render(scene, camera);
              }
              animate();
            }, undefined, function (error) {
              console.error(`Error loading GLB file: ${error}`);
            });
    
            camera.position.z = 2;
          }
    
          function loadInteractiveModel_mtl(model, index, viewer) {
            while (viewer.firstChild) {
              viewer.removeChild(viewer.firstChild);
            }
    
            if (interactiveRenderer) {
              interactiveRenderer.dispose();
              interactiveRenderer.forceContextLoss();
              interactiveRenderer = null;
            }
    
            const scene = new THREE.Scene();
            const camera = new THREE.PerspectiveCamera(50, viewer.offsetWidth / viewer.offsetHeight, 0.1, 1000);
            interactiveRenderer = new THREE.WebGLRenderer({ antialias: true });
            interactiveRenderer.setSize(viewer.offsetWidth, viewer.offsetHeight);
            viewer.appendChild(interactiveRenderer.domElement);
    
            interactiveRenderer.setClearColor(0xCCCCCC);
    
            const controls = new OrbitControls(camera, interactiveRenderer.domElement);
            controls.enableDamping = true;
            controls.dampingFactor = 0.25;
            controls.screenSpacePanning = false;
            controls.minDistance = 1;
            controls.maxDistance = 1000;
    
            const ambientLight = new THREE.AmbientLight(0x404040, 8);
            scene.add(ambientLight);
    
            const directionalLight = new THREE.DirectionalLight(0xffffff, 8);
            directionalLight.position.set(0, 1, 1).normalize();
            scene.add(directionalLight);
    
            const mtlLoader = new MTLLoader();
            mtlLoader.setPath(`files/obj/${model}/`);
            mtlLoader.load('base.mtl', (materials) => {
              materials.preload();
              const objLoader = new OBJLoader();
              objLoader.setMaterials(materials);
              objLoader.setPath(`files/obj/${model}/`);
              objLoader.load('textured.obj', function (object) {
                scene.add(object);
                object.position.set(0, 0, 0);
    
                const box = new THREE.Box3().setFromObject(object);
                const center = box.getCenter(new THREE.Vector3());
                const size = box.getSize(new THREE.Vector3());
    
                const maxDim = Math.max(size.x, size.y, size.z);
                const scale = 1 / maxDim;
                
                if (index == 1 || index == 4){
                  object.scale.set(7, 7, 7);
                }
                  
                camera.position.set(center.x, center.y, size.z * 2);
                camera.lookAt(center);
    
                function animate() {
                  requestAnimationFrame(animate);
                  controls.update();
                  directionalLight.position.copy(camera.position).add(new THREE.Vector3(0, 1, 1));
                  interactiveRenderer.render(scene, camera);
                }
                animate();
              }, undefined, function (error) {
                console.error(`Error loading OBJ file: ${error}`);
              });
            }, undefined, function (error) {
              console.error(`Error loading MTL file: ${error}`);
            });
    
            camera.position.z = 2;
          }
    
          function loadInteractiveModel_glb(model, index, viewer) {
            while (viewer.firstChild) {
              viewer.removeChild(viewer.firstChild);
            }
    
            if (interactiveRenderer) {
              interactiveRenderer.dispose();
              interactiveRenderer.forceContextLoss();
              interactiveRenderer = null;
            }
    
            const scene = new THREE.Scene();
            const camera = new THREE.PerspectiveCamera(50, viewer.offsetWidth / viewer.offsetHeight, 0.1, 1000);
            interactiveRenderer = new THREE.WebGLRenderer({ antialias: true });
            interactiveRenderer.setSize(viewer.offsetWidth, viewer.offsetHeight);
            interactiveRenderer.outputEncoding = THREE.sRGBEncoding;
            viewer.appendChild(interactiveRenderer.domElement);
    
            interactiveRenderer.setClearColor(0xCCCCCC);
    
            const controls = new OrbitControls(camera, interactiveRenderer.domElement);
            controls.enableDamping = true;
            controls.dampingFactor = 0.25;
            controls.screenSpacePanning = false;
            controls.minDistance = 1;
            controls.maxDistance = 1000;
    
            const ambientLight = new THREE.AmbientLight(0x404040, 6);
            scene.add(ambientLight);
    
            const directionalLight = new THREE.DirectionalLight(0xffffff, 2);
            directionalLight.position.set(0, 1, 1).normalize();
            scene.add(directionalLight);
    
            const gltfLoader = new GLTFLoader();
            gltfLoader.load(`files/glb/${model}/base.glb`, function (gltf) {
              const object = gltf.scene;
              scene.add(object);
              object.position.set(0, 0, 0);
              object.traverse((node) => {
                if (node.isMesh) {
                  node.material = new THREE.MeshStandardMaterial({
                    map: node.material.map,
                    roughness: node.material.roughness,
                    metalness: node.material.metalness,
                  });
                }
              });
              const box = new THREE.Box3().setFromObject(object);
              const center = box.getCenter(new THREE.Vector3());
              const size = box.getSize(new THREE.Vector3());
    
              const maxDim = Math.max(size.x, size.y, size.z);
              const scale = 1 / maxDim;
              // object.scale.set(scalesize[index], scalesize[index], scalesize[index]);
    
              camera.position.set(center.x, center.y, size.z * 2);
              camera.lookAt(center);
    
              function animate() {
                requestAnimationFrame(animate);
                controls.update();
                directionalLight.position.copy(camera.position).add(new THREE.Vector3(0, 1, 1));
                interactiveRenderer.render(scene, camera);
              }
              animate();
            }, undefined, function (error) {
              console.error(`Error loading GLB file: ${error}`);
            });
    
            camera.position.z = 2;
          }
    
        });
      </script>
<!--     <section class="acknowledgements">-->
<!--       <h2>Acknowledgements</h2>-->
<!--       <p style="font-family: 'Times New Roman', Arial;">-->
<!--        The authors extend their profound gratitude to <strong>D-robotics</strong> for their invaluable support in supplying the necessary cloud computing resources that facilitated the execution of this research. Furthermore, our sincere appreciation is extended to Deeoms for their contribution in providing essential model support, which was pivotal to the successful completion of this study.-->
<!--        </p> -->
<!--      </section>-->
    </main>
  </body>
</html>
